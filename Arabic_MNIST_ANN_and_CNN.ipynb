{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Arabic_MNIST_ANN_and_CNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMiSRH7RAwJMJNNkNGBlYHt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rayenmbarek/Deep-Learning/blob/main/Arabic_MNIST_ANN_and_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-1s4Efa2Ycm"
      },
      "source": [
        "# Python packages to manipulate files\n",
        "import os\n",
        "import pathlib\n",
        "from pathlib import Path\n",
        "import datetime\n",
        "import shutil\n",
        "\n",
        "# Tensorflow, Keras and Numpy packages\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import keras_preprocessing\n",
        "from keras_preprocessing import image\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Display related packages\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from IPython.display import Image\n",
        "import PIL\n",
        "import PIL.Image\n",
        "\n",
        "%matplotlib inline\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hughyjh_3VVp"
      },
      "source": [
        "my_devices = tf.config.experimental.list_physical_devices(device_type='CPU')\n",
        "tf.config.experimental.set_visible_devices(devices= my_devices, device_type='CPU')\n",
        "# #tf.config.set_visible_devices([], 'GPU')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zVs1zuM3VX5"
      },
      "source": [
        "! git clone https://github.com/minus--/arabic-letters-tutorial"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afq4lORg3VdV"
      },
      "source": [
        "! tar xvzf arabic-letters-tutorial/arabic_handwritten_data.tgz # unzip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZf87f0C50pr"
      },
      "source": [
        "# ***Create New Folders***\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hchut2aA3Vgx"
      },
      "source": [
        "mkdir data/new_data"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IF35cl-63VjK"
      },
      "source": [
        "mkdir data/new_data/test_data"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DL9xQ1v3Vl4"
      },
      "source": [
        "mkdir data/new_data/train_data"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ja3asi-33Vqh"
      },
      "source": [
        "for i in range(1,29):\n",
        "  os.mkdir(\"data/new_data/test_data/{}\".format(i))\n",
        "  os.mkdir(\"data/new_data/train_data/{}\".format(i))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8eq6OF76BVl"
      },
      "source": [
        "# ***Copy the classified data into the new folders***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11alKzEE3Vsf"
      },
      "source": [
        "directory_train = \"./data/train_data/\"\n",
        "images_train = os.listdir(directory_train)\n",
        "directory_test = \"./data/test_data/\"\n",
        "images_test = os.listdir(directory_test)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zpt00pj63Vu2"
      },
      "source": [
        "for image in images_train:\n",
        "  if image.endswith('.png'):\n",
        "    label =int(tf.strings.regex_replace(input=image,pattern=r\".+_label_(\\d+)\\.png\",rewrite =r\"\\1\").numpy())\n",
        "    original = directory_train+image\n",
        "    target =(\"./data/new_data/train_data/{}/\".format(label))+image\n",
        "    shutil.copy2(original,target)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TE9bl3a94FUL"
      },
      "source": [
        "for image in images_test:\n",
        "  if image.endswith('.png'):\n",
        "    label =int(tf.strings.regex_replace(input=image,pattern=r\".+_label_(\\d+)\\.png\",rewrite =r\"\\1\").numpy())\n",
        "    original = directory_test+image\n",
        "    target =(\"./data/new_data/test_data/{}/\".format(label))+image\n",
        "    shutil.copy2(original,target)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtwdXxyP6LG8"
      },
      "source": [
        "# ***CNN MODEL***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tw5dw1YY4FW8",
        "outputId": "9cb2f046-eb17-49fb-f02b-6e522c1de907"
      },
      "source": [
        "TRAINING_DIR = \"data/new_data/train_data/\"\n",
        "training_datagen = ImageDataGenerator(\n",
        "      rescale = 1./255,\n",
        "      rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        "\n",
        "VALIDATION_DIR = \"data/new_data/test_data/\"\n",
        "validation_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "train_generator = training_datagen.flow_from_directory(\n",
        "  TRAINING_DIR,\n",
        "  target_size=(32,32),\n",
        "  class_mode='categorical',\n",
        "  batch_size=32\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "  VALIDATION_DIR,\n",
        "  target_size=(32,32),\n",
        "  class_mode='categorical',\n",
        "  batch_size=32\n",
        ")\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n",
        "    # This is the first convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu', padding=\"same\", input_shape=(32, 32, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    # The second convolution\n",
        "    #tf.keras.layers.Conv2D(64, (3,3), activation='relu', padding=\"same\"),\n",
        "    #tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The third convolution\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu', padding=\"same\"),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The fourth convolution\n",
        "    tf.keras.layers.Conv2D(256, (3,3), activation='relu', padding=\"same\"),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Flatten the results to feed into a DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    # 512 neuron hidden layer\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(28, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer='rmsprop',metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_generator, epochs=50, steps_per_epoch=420, validation_data = validation_generator, verbose = 1, validation_steps=105)\n",
        "\n",
        "model.save(\"data_new.h5\")\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 13440 images belonging to 28 classes.\n",
            "Found 3360 images belonging to 28 classes.\n",
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_44 (Conv2D)           (None, 32, 32, 64)        1792      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_44 (MaxPooling (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_45 (Conv2D)           (None, 16, 16, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_45 (MaxPooling (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_46 (Conv2D)           (None, 8, 8, 256)         295168    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_46 (MaxPooling (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_12 (Flatten)         (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 512)               2097664   \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 28)                14364     \n",
            "=================================================================\n",
            "Total params: 2,482,844\n",
            "Trainable params: 2,482,844\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "420/420 [==============================] - 108s 256ms/step - loss: 2.7413 - accuracy: 0.1921 - val_loss: 1.4778 - val_accuracy: 0.5155\n",
            "Epoch 2/50\n",
            "420/420 [==============================] - 113s 269ms/step - loss: 1.8501 - accuracy: 0.4254 - val_loss: 1.0966 - val_accuracy: 0.6295\n",
            "Epoch 3/50\n",
            "420/420 [==============================] - 108s 257ms/step - loss: 1.4732 - accuracy: 0.5324 - val_loss: 0.6811 - val_accuracy: 0.7872\n",
            "Epoch 4/50\n",
            "420/420 [==============================] - 108s 256ms/step - loss: 1.2575 - accuracy: 0.5999 - val_loss: 1.1857 - val_accuracy: 0.6268\n",
            "Epoch 5/50\n",
            "420/420 [==============================] - 108s 256ms/step - loss: 1.1127 - accuracy: 0.6517 - val_loss: 1.2423 - val_accuracy: 0.6060\n",
            "Epoch 6/50\n",
            "420/420 [==============================] - 108s 258ms/step - loss: 1.0240 - accuracy: 0.6760 - val_loss: 0.9502 - val_accuracy: 0.7077\n",
            "Epoch 7/50\n",
            "420/420 [==============================] - 108s 257ms/step - loss: 0.9371 - accuracy: 0.7036 - val_loss: 0.8845 - val_accuracy: 0.7357\n",
            "Epoch 8/50\n",
            "420/420 [==============================] - 112s 267ms/step - loss: 0.9109 - accuracy: 0.7118 - val_loss: 0.7012 - val_accuracy: 0.7899\n",
            "Epoch 9/50\n",
            "420/420 [==============================] - 108s 257ms/step - loss: 0.8629 - accuracy: 0.7286 - val_loss: 0.7810 - val_accuracy: 0.7655\n",
            "Epoch 10/50\n",
            "420/420 [==============================] - 108s 258ms/step - loss: 0.8315 - accuracy: 0.7398 - val_loss: 1.0565 - val_accuracy: 0.7253\n",
            "Epoch 11/50\n",
            "420/420 [==============================] - 108s 258ms/step - loss: 0.8239 - accuracy: 0.7417 - val_loss: 1.1510 - val_accuracy: 0.6955\n",
            "Epoch 12/50\n",
            "420/420 [==============================] - 108s 258ms/step - loss: 0.7963 - accuracy: 0.7469 - val_loss: 0.7025 - val_accuracy: 0.8039\n",
            "Epoch 13/50\n",
            "420/420 [==============================] - 108s 257ms/step - loss: 0.7945 - accuracy: 0.7557 - val_loss: 0.6099 - val_accuracy: 0.8420\n",
            "Epoch 14/50\n",
            "420/420 [==============================] - 108s 257ms/step - loss: 0.7702 - accuracy: 0.7634 - val_loss: 0.8011 - val_accuracy: 0.7878\n",
            "Epoch 15/50\n",
            "420/420 [==============================] - 108s 258ms/step - loss: 0.7669 - accuracy: 0.7665 - val_loss: 2.0800 - val_accuracy: 0.5961\n",
            "Epoch 16/50\n",
            "420/420 [==============================] - 108s 258ms/step - loss: 0.7519 - accuracy: 0.7717 - val_loss: 1.1435 - val_accuracy: 0.7348\n",
            "Epoch 17/50\n",
            "420/420 [==============================] - 108s 258ms/step - loss: 0.7444 - accuracy: 0.7737 - val_loss: 1.0900 - val_accuracy: 0.7696\n",
            "Epoch 18/50\n",
            "420/420 [==============================] - 108s 258ms/step - loss: 0.7593 - accuracy: 0.7742 - val_loss: 2.3727 - val_accuracy: 0.5750\n",
            "Epoch 19/50\n",
            "420/420 [==============================] - 108s 258ms/step - loss: 0.7532 - accuracy: 0.7691 - val_loss: 0.8378 - val_accuracy: 0.8190\n",
            "Epoch 20/50\n",
            "420/420 [==============================] - 109s 259ms/step - loss: 0.7455 - accuracy: 0.7740 - val_loss: 1.2228 - val_accuracy: 0.7485\n",
            "Epoch 21/50\n",
            "420/420 [==============================] - 108s 258ms/step - loss: 0.7528 - accuracy: 0.7751 - val_loss: 0.8935 - val_accuracy: 0.7988\n",
            "Epoch 22/50\n",
            "420/420 [==============================] - 108s 257ms/step - loss: 0.7628 - accuracy: 0.7693 - val_loss: 2.1354 - val_accuracy: 0.7360\n",
            "Epoch 23/50\n",
            "420/420 [==============================] - 108s 257ms/step - loss: 0.7635 - accuracy: 0.7728 - val_loss: 1.3517 - val_accuracy: 0.7946\n",
            "Epoch 24/50\n",
            "420/420 [==============================] - 108s 257ms/step - loss: 0.7542 - accuracy: 0.7717 - val_loss: 1.6391 - val_accuracy: 0.7729\n",
            "Epoch 25/50\n",
            "420/420 [==============================] - 108s 257ms/step - loss: 0.7724 - accuracy: 0.7717 - val_loss: 0.9446 - val_accuracy: 0.8381\n",
            "Epoch 26/50\n",
            "420/420 [==============================] - 108s 258ms/step - loss: 0.7641 - accuracy: 0.7747 - val_loss: 0.6990 - val_accuracy: 0.8655\n",
            "Epoch 27/50\n",
            "420/420 [==============================] - 108s 257ms/step - loss: 0.7802 - accuracy: 0.7723 - val_loss: 1.2785 - val_accuracy: 0.8247\n",
            "Epoch 28/50\n",
            "420/420 [==============================] - 108s 256ms/step - loss: 0.7840 - accuracy: 0.7706 - val_loss: 0.8681 - val_accuracy: 0.8533\n",
            "Epoch 29/50\n",
            "420/420 [==============================] - 108s 256ms/step - loss: 0.7764 - accuracy: 0.7743 - val_loss: 2.2325 - val_accuracy: 0.7857\n",
            "Epoch 30/50\n",
            "420/420 [==============================] - 107s 256ms/step - loss: 0.7928 - accuracy: 0.7675 - val_loss: 1.2473 - val_accuracy: 0.8482\n",
            "Epoch 31/50\n",
            "420/420 [==============================] - 108s 257ms/step - loss: 0.7793 - accuracy: 0.7668 - val_loss: 1.3169 - val_accuracy: 0.8244\n",
            "Epoch 32/50\n",
            "420/420 [==============================] - 107s 256ms/step - loss: 0.7974 - accuracy: 0.7700 - val_loss: 1.4481 - val_accuracy: 0.8202\n",
            "Epoch 33/50\n",
            "420/420 [==============================] - 108s 256ms/step - loss: 0.7997 - accuracy: 0.7671 - val_loss: 0.9803 - val_accuracy: 0.8449\n",
            "Epoch 34/50\n",
            "420/420 [==============================] - 108s 256ms/step - loss: 0.8086 - accuracy: 0.7641 - val_loss: 2.2654 - val_accuracy: 0.7307\n",
            "Epoch 35/50\n",
            "420/420 [==============================] - 107s 256ms/step - loss: 0.8048 - accuracy: 0.7612 - val_loss: 1.1907 - val_accuracy: 0.8298\n",
            "Epoch 36/50\n",
            "420/420 [==============================] - 107s 256ms/step - loss: 0.8021 - accuracy: 0.7668 - val_loss: 2.0259 - val_accuracy: 0.7539\n",
            "Epoch 37/50\n",
            "420/420 [==============================] - 107s 256ms/step - loss: 0.7982 - accuracy: 0.7670 - val_loss: 2.8716 - val_accuracy: 0.7958\n",
            "Epoch 38/50\n",
            "420/420 [==============================] - 108s 256ms/step - loss: 0.8252 - accuracy: 0.7652 - val_loss: 1.9615 - val_accuracy: 0.7705\n",
            "Epoch 39/50\n",
            "420/420 [==============================] - 107s 255ms/step - loss: 0.8148 - accuracy: 0.7592 - val_loss: 7.5152 - val_accuracy: 0.6804\n",
            "Epoch 40/50\n",
            "420/420 [==============================] - 108s 257ms/step - loss: 0.8249 - accuracy: 0.7582 - val_loss: 1.5978 - val_accuracy: 0.8188\n",
            "Epoch 41/50\n",
            "420/420 [==============================] - 108s 257ms/step - loss: 0.8553 - accuracy: 0.7560 - val_loss: 2.0396 - val_accuracy: 0.7667\n",
            "Epoch 42/50\n",
            "420/420 [==============================] - 108s 256ms/step - loss: 0.8608 - accuracy: 0.7528 - val_loss: 1.9605 - val_accuracy: 0.7807\n",
            "Epoch 43/50\n",
            "420/420 [==============================] - 108s 257ms/step - loss: 0.8556 - accuracy: 0.7560 - val_loss: 2.4745 - val_accuracy: 0.7670\n",
            "Epoch 44/50\n",
            "420/420 [==============================] - 108s 257ms/step - loss: 0.8301 - accuracy: 0.7580 - val_loss: 1.0853 - val_accuracy: 0.8057\n",
            "Epoch 45/50\n",
            "420/420 [==============================] - 112s 267ms/step - loss: 0.8598 - accuracy: 0.7536 - val_loss: 3.3554 - val_accuracy: 0.7554\n",
            "Epoch 46/50\n",
            "420/420 [==============================] - 108s 258ms/step - loss: 0.8864 - accuracy: 0.7451 - val_loss: 1.2209 - val_accuracy: 0.8244\n",
            "Epoch 47/50\n",
            "420/420 [==============================] - 108s 257ms/step - loss: 0.8502 - accuracy: 0.7551 - val_loss: 2.6165 - val_accuracy: 0.7417\n",
            "Epoch 48/50\n",
            "420/420 [==============================] - 109s 259ms/step - loss: 0.8717 - accuracy: 0.7493 - val_loss: 1.4851 - val_accuracy: 0.8384\n",
            "Epoch 49/50\n",
            "420/420 [==============================] - 109s 259ms/step - loss: 0.8348 - accuracy: 0.7575 - val_loss: 2.0290 - val_accuracy: 0.8042\n",
            "Epoch 50/50\n",
            "420/420 [==============================] - 114s 271ms/step - loss: 0.8632 - accuracy: 0.7499 - val_loss: 3.0963 - val_accuracy: 0.7345\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3ECarB38pcZ"
      },
      "source": [
        "# ***Loading data for ANN (MLP)***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1ugmT1J4Fb9"
      },
      "source": [
        "batch_size = 32\n",
        "img_height = 32\n",
        "img_width = 32\n",
        "\n",
        "def get_dataset(dataset_dir):\n",
        "    \n",
        "    def process_filename(file_path):\n",
        "        label = tf.strings.regex_replace(input=file_path,pattern=r\".+_label_(\\d+)\\.png\", rewrite=r\"\\1\")\n",
        "        label = tf.strings.to_number(label, tf.int32)-1\n",
        "        #label = tf.one_hot(label, depth=29)\n",
        "        return label\n",
        "\n",
        "    def process_img(file_path):\n",
        "\n",
        "        img = tf.io.read_file(file_path)\n",
        "        img = tf.image.decode_png(img, channels=3)\n",
        "        img = tf.image.resize(img, size=(32, 32))\n",
        "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "        img = tf.cast(img, tf.float32) / 255.0\n",
        "        return img\n",
        "    \n",
        "    data_dir = pathlib.Path(dataset_dir)\n",
        "    file_list = [str(path.absolute()) for path in Path(data_dir).glob(\"*.png\")]\n",
        "    files_ds = tf.data.Dataset.from_tensor_slices((file_list))\n",
        "    files_ds = files_ds.map(lambda x: (process_img(x), process_filename(x)))\n",
        "    return files_ds"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sS9QYviT4FfI"
      },
      "source": [
        "train_dataset_path = \"data/train_data\"\n",
        "test_dataset_path = \"data/test_data\"\n",
        "\n",
        "train_ds = get_dataset(train_dataset_path).shuffle(buffer_size=batch_size*10).batch(batch_size)\n",
        "valid_ds = get_dataset(test_dataset_path).batch(batch_size)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3XKtG9D85lM"
      },
      "source": [
        "# **ANN MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xrLN9Xi4FaV",
        "outputId": "712ff4d5-e556-4ae0-9dba-b993d5cadd4c"
      },
      "source": [
        "model1 = tf.keras.Sequential([\n",
        "    # Convert the 32x32x3 image into a flat vector of 32x32x3 = 3072 values\n",
        "    tf.keras.layers.Flatten(input_shape=(32, 32, 3), name='flatten_input'),\n",
        "    # Create a \"hidden\" layer with 256 neurons and apply the ReLU non-linearity\n",
        "    tf.keras.layers.Dense(256, activation=tf.nn.relu, name='input_to_hidden1'),\n",
        "    # Create another hidden layer with 128 neurons\n",
        "    tf.keras.layers.Dense(128, activation=tf.nn.relu, name='hidden1_to_hidden2'),\n",
        "    # Create an \"output layer\" with 28 neurons\n",
        "    tf.keras.layers.Dense(28, name='hidden_to_logits'),\n",
        "])\n",
        "model1.summary()\n",
        "model1.compile(\n",
        "    # Optimizer\n",
        "    optimizer=tf.keras.optimizers.RMSprop(),  \n",
        "    # Loss function to minimize\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    # List of metrics to monitor\n",
        "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "history = model1.fit(train_ds, epochs = 50, validation_data=valid_ds)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_input (Flatten)      (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "input_to_hidden1 (Dense)     (None, 256)               786688    \n",
            "_________________________________________________________________\n",
            "hidden1_to_hidden2 (Dense)   (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "hidden_to_logits (Dense)     (None, 28)                3612      \n",
            "=================================================================\n",
            "Total params: 823,196\n",
            "Trainable params: 823,196\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "420/420 [==============================] - 10s 24ms/step - loss: 1.7992 - sparse_categorical_accuracy: 0.4481 - val_loss: 1.2146 - val_sparse_categorical_accuracy: 0.5973\n",
            "Epoch 2/50\n",
            "420/420 [==============================] - 9s 22ms/step - loss: 0.9323 - sparse_categorical_accuracy: 0.6923 - val_loss: 1.0259 - val_sparse_categorical_accuracy: 0.6586\n",
            "Epoch 3/50\n",
            "420/420 [==============================] - 9s 22ms/step - loss: 0.6205 - sparse_categorical_accuracy: 0.7929 - val_loss: 0.9215 - val_sparse_categorical_accuracy: 0.7131\n",
            "Epoch 4/50\n",
            "420/420 [==============================] - 9s 23ms/step - loss: 0.4448 - sparse_categorical_accuracy: 0.8525 - val_loss: 0.8820 - val_sparse_categorical_accuracy: 0.7423\n",
            "Epoch 5/50\n",
            "420/420 [==============================] - 9s 23ms/step - loss: 0.3165 - sparse_categorical_accuracy: 0.8932 - val_loss: 0.9685 - val_sparse_categorical_accuracy: 0.7274\n",
            "Epoch 6/50\n",
            "420/420 [==============================] - 9s 22ms/step - loss: 0.2305 - sparse_categorical_accuracy: 0.9219 - val_loss: 0.9794 - val_sparse_categorical_accuracy: 0.7476\n",
            "Epoch 7/50\n",
            "420/420 [==============================] - 10s 24ms/step - loss: 0.1717 - sparse_categorical_accuracy: 0.9410 - val_loss: 1.0566 - val_sparse_categorical_accuracy: 0.7420\n",
            "Epoch 8/50\n",
            "420/420 [==============================] - 10s 23ms/step - loss: 0.1265 - sparse_categorical_accuracy: 0.9561 - val_loss: 1.1319 - val_sparse_categorical_accuracy: 0.7512\n",
            "Epoch 9/50\n",
            "420/420 [==============================] - 9s 22ms/step - loss: 0.0985 - sparse_categorical_accuracy: 0.9658 - val_loss: 1.2137 - val_sparse_categorical_accuracy: 0.7563\n",
            "Epoch 10/50\n",
            "420/420 [==============================] - 9s 22ms/step - loss: 0.0808 - sparse_categorical_accuracy: 0.9719 - val_loss: 1.3171 - val_sparse_categorical_accuracy: 0.7613\n",
            "Epoch 11/50\n",
            "420/420 [==============================] - 9s 22ms/step - loss: 0.0620 - sparse_categorical_accuracy: 0.9800 - val_loss: 1.3820 - val_sparse_categorical_accuracy: 0.7613\n",
            "Epoch 12/50\n",
            "420/420 [==============================] - 9s 22ms/step - loss: 0.0580 - sparse_categorical_accuracy: 0.9802 - val_loss: 1.5113 - val_sparse_categorical_accuracy: 0.7568\n",
            "Epoch 13/50\n",
            "420/420 [==============================] - 9s 22ms/step - loss: 0.0415 - sparse_categorical_accuracy: 0.9864 - val_loss: 1.5856 - val_sparse_categorical_accuracy: 0.7583\n",
            "Epoch 14/50\n",
            "420/420 [==============================] - 9s 22ms/step - loss: 0.0363 - sparse_categorical_accuracy: 0.9873 - val_loss: 1.6871 - val_sparse_categorical_accuracy: 0.7637\n",
            "Epoch 15/50\n",
            "420/420 [==============================] - 9s 22ms/step - loss: 0.0358 - sparse_categorical_accuracy: 0.9874 - val_loss: 1.6972 - val_sparse_categorical_accuracy: 0.7554\n",
            "Epoch 16/50\n",
            "420/420 [==============================] - 9s 22ms/step - loss: 0.0241 - sparse_categorical_accuracy: 0.9923 - val_loss: 1.7922 - val_sparse_categorical_accuracy: 0.7667\n",
            "Epoch 17/50\n",
            "420/420 [==============================] - 9s 22ms/step - loss: 0.0239 - sparse_categorical_accuracy: 0.9926 - val_loss: 1.8770 - val_sparse_categorical_accuracy: 0.7545\n",
            "Epoch 18/50\n",
            "420/420 [==============================] - 9s 22ms/step - loss: 0.0212 - sparse_categorical_accuracy: 0.9932 - val_loss: 1.9894 - val_sparse_categorical_accuracy: 0.7604\n",
            "Epoch 19/50\n",
            "420/420 [==============================] - 9s 22ms/step - loss: 0.0204 - sparse_categorical_accuracy: 0.9930 - val_loss: 2.0414 - val_sparse_categorical_accuracy: 0.7586\n",
            "Epoch 20/50\n",
            "420/420 [==============================] - 9s 22ms/step - loss: 0.0206 - sparse_categorical_accuracy: 0.9932 - val_loss: 2.1648 - val_sparse_categorical_accuracy: 0.7661\n",
            "Epoch 21/50\n",
            "420/420 [==============================] - 9s 22ms/step - loss: 0.0182 - sparse_categorical_accuracy: 0.9943 - val_loss: 2.1442 - val_sparse_categorical_accuracy: 0.7765\n",
            "Epoch 22/50\n",
            "420/420 [==============================] - 9s 22ms/step - loss: 0.0148 - sparse_categorical_accuracy: 0.9956 - val_loss: 2.1884 - val_sparse_categorical_accuracy: 0.7661\n",
            "Epoch 23/50\n",
            "420/420 [==============================] - 9s 22ms/step - loss: 0.0159 - sparse_categorical_accuracy: 0.9942 - val_loss: 2.1706 - val_sparse_categorical_accuracy: 0.7792\n",
            "Epoch 24/50\n",
            "420/420 [==============================] - 9s 22ms/step - loss: 0.0168 - sparse_categorical_accuracy: 0.9947 - val_loss: 2.3911 - val_sparse_categorical_accuracy: 0.7601\n",
            "Epoch 25/50\n",
            "420/420 [==============================] - 9s 22ms/step - loss: 0.0116 - sparse_categorical_accuracy: 0.9955 - val_loss: 2.4290 - val_sparse_categorical_accuracy: 0.7714\n",
            "Epoch 26/50\n",
            "420/420 [==============================] - 9s 22ms/step - loss: 0.0101 - sparse_categorical_accuracy: 0.9967 - val_loss: 2.4886 - val_sparse_categorical_accuracy: 0.7705\n",
            "Epoch 27/50\n",
            "420/420 [==============================] - 9s 22ms/step - loss: 0.0116 - sparse_categorical_accuracy: 0.9964 - val_loss: 2.4660 - val_sparse_categorical_accuracy: 0.7667\n",
            "Epoch 28/50\n",
            "420/420 [==============================] - 9s 22ms/step - loss: 0.0153 - sparse_categorical_accuracy: 0.9959 - val_loss: 2.6692 - val_sparse_categorical_accuracy: 0.7601\n",
            "Epoch 29/50\n",
            "420/420 [==============================] - 9s 23ms/step - loss: 0.0147 - sparse_categorical_accuracy: 0.9951 - val_loss: 2.5550 - val_sparse_categorical_accuracy: 0.7732\n",
            "Epoch 30/50\n",
            "420/420 [==============================] - 9s 22ms/step - loss: 0.0114 - sparse_categorical_accuracy: 0.9959 - val_loss: 2.6420 - val_sparse_categorical_accuracy: 0.7756\n",
            "Epoch 31/50\n",
            "420/420 [==============================] - 9s 22ms/step - loss: 0.0097 - sparse_categorical_accuracy: 0.9968 - val_loss: 2.7039 - val_sparse_categorical_accuracy: 0.7702\n",
            "Epoch 32/50\n",
            "420/420 [==============================] - 9s 22ms/step - loss: 0.0093 - sparse_categorical_accuracy: 0.9969 - val_loss: 2.8026 - val_sparse_categorical_accuracy: 0.7723\n",
            "Epoch 33/50\n",
            "420/420 [==============================] - 9s 22ms/step - loss: 0.0098 - sparse_categorical_accuracy: 0.9972 - val_loss: 2.9139 - val_sparse_categorical_accuracy: 0.7759\n",
            "Epoch 34/50\n",
            "420/420 [==============================] - 9s 22ms/step - loss: 0.0113 - sparse_categorical_accuracy: 0.9961 - val_loss: 2.8830 - val_sparse_categorical_accuracy: 0.7750\n",
            "Epoch 35/50\n",
            "420/420 [==============================] - 9s 22ms/step - loss: 0.0099 - sparse_categorical_accuracy: 0.9966 - val_loss: 3.0024 - val_sparse_categorical_accuracy: 0.7741\n",
            "Epoch 36/50\n",
            "420/420 [==============================] - 9s 22ms/step - loss: 0.0109 - sparse_categorical_accuracy: 0.9967 - val_loss: 3.0797 - val_sparse_categorical_accuracy: 0.7688\n",
            "Epoch 37/50\n",
            "420/420 [==============================] - 9s 22ms/step - loss: 0.0075 - sparse_categorical_accuracy: 0.9970 - val_loss: 3.0850 - val_sparse_categorical_accuracy: 0.7705\n",
            "Epoch 38/50\n",
            "420/420 [==============================] - 9s 22ms/step - loss: 0.0087 - sparse_categorical_accuracy: 0.9978 - val_loss: 3.1392 - val_sparse_categorical_accuracy: 0.7759\n",
            "Epoch 39/50\n",
            "420/420 [==============================] - 9s 22ms/step - loss: 0.0078 - sparse_categorical_accuracy: 0.9977 - val_loss: 3.4024 - val_sparse_categorical_accuracy: 0.7625\n",
            "Epoch 40/50\n",
            "420/420 [==============================] - 10s 23ms/step - loss: 0.0069 - sparse_categorical_accuracy: 0.9978 - val_loss: 3.3576 - val_sparse_categorical_accuracy: 0.7723\n",
            "Epoch 41/50\n",
            "420/420 [==============================] - 9s 22ms/step - loss: 0.0071 - sparse_categorical_accuracy: 0.9975 - val_loss: 3.3553 - val_sparse_categorical_accuracy: 0.7696\n",
            "Epoch 42/50\n",
            "420/420 [==============================] - 9s 22ms/step - loss: 0.0047 - sparse_categorical_accuracy: 0.9983 - val_loss: 3.5837 - val_sparse_categorical_accuracy: 0.7643\n",
            "Epoch 43/50\n",
            "420/420 [==============================] - 9s 22ms/step - loss: 0.0068 - sparse_categorical_accuracy: 0.9979 - val_loss: 3.3143 - val_sparse_categorical_accuracy: 0.7744\n",
            "Epoch 44/50\n",
            "420/420 [==============================] - 9s 22ms/step - loss: 0.0092 - sparse_categorical_accuracy: 0.9978 - val_loss: 3.4987 - val_sparse_categorical_accuracy: 0.7673\n",
            "Epoch 45/50\n",
            "420/420 [==============================] - 9s 22ms/step - loss: 0.0055 - sparse_categorical_accuracy: 0.9985 - val_loss: 3.5064 - val_sparse_categorical_accuracy: 0.7708\n",
            "Epoch 46/50\n",
            "420/420 [==============================] - 9s 22ms/step - loss: 0.0057 - sparse_categorical_accuracy: 0.9983 - val_loss: 3.5182 - val_sparse_categorical_accuracy: 0.7711\n",
            "Epoch 47/50\n",
            "420/420 [==============================] - 9s 22ms/step - loss: 0.0056 - sparse_categorical_accuracy: 0.9987 - val_loss: 3.5666 - val_sparse_categorical_accuracy: 0.7777\n",
            "Epoch 48/50\n",
            "420/420 [==============================] - 9s 22ms/step - loss: 0.0056 - sparse_categorical_accuracy: 0.9984 - val_loss: 3.6652 - val_sparse_categorical_accuracy: 0.7744\n",
            "Epoch 49/50\n",
            "420/420 [==============================] - 9s 22ms/step - loss: 0.0054 - sparse_categorical_accuracy: 0.9981 - val_loss: 3.7365 - val_sparse_categorical_accuracy: 0.7640\n",
            "Epoch 50/50\n",
            "420/420 [==============================] - 9s 22ms/step - loss: 0.0067 - sparse_categorical_accuracy: 0.9977 - val_loss: 3.8495 - val_sparse_categorical_accuracy: 0.7735\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3v0bOxxgQwN"
      },
      "source": [
        "# ***Comparaison Between ANN (mlp) and CNN for arabic_MNIST***\n",
        "\n",
        "ANN_accuracy = 0.7735 and CNN_accuracy = 0.7345\n",
        "ANN_loss = 3.8495 and CNN_loss = 3.0963\n",
        "\n",
        "Even for ANN is better than CNN in accuracy for the validation dataset but in the loss we found that CNN is lower than ANN loss, and there is not a big diffrence between the accuracy, so we can say that CNN is better for arabic_MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeuL-8zg3Vys"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xA9pTNj3V01"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}